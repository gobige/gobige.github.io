## 计算机计算

- 加法器：二进制加法和我平常使用加法一样，进位。
- 乘法器：乘数和被乘数每一位分别相乘，结果相加。时间复杂度0(N) N是数的位数。**并行加速实现**，时间复杂度O（logN）。

通过精巧的设计电路，用较少的门电路和寄存器，就能完成乘法这样相对复杂的运算，但是需要更长的门延迟和时钟周期；通过复杂的电路可以实现短的门延迟和时钟周期

## 浮点和定点数

**定点数**
现在使用计算机通常32位，最多表示40亿个数

32个bit，4bit作为一个整数（0~9），最右边2个整数表示小数，左边6个整数表示整数部分，可以实现0~999999.99之间数字。(这种二进制表示十进制方法叫做**BCD编码**，缺点：能表示范围有限；)

**浮点数**
空间容量限制了能表示数的大小，那么就只能使用**科学计数法**来表示。计算机使用IEEE标准

单精度
![](https://yatesblog.oss-cn-shenzhen.aliyuncs.com/img/computer-system-Perspective/39.png)

- 符号位：s表示正数还是负数
- 指数位：e用来表示指数，-127~127
- 有效位数：f表示有效位数，23位

当e为0且f为0是表示数字0，其他情况晖表示为无穷大，无穷小
![](https://yatesblog.oss-cn-shenzhen.aliyuncs.com/img/computer-system-Perspective/40.png)

**浮点数十进制转二进制**

- 整数部分：和整数十进制转二进制一致
- 小数部分：不断乘以2，如果超过1，就记下1，并把结果减去1，进一步循环操作。所以很多10进制小数都会无效循环下去。而计算机作为有效位只会截取到23位

**浮点数加法**

两个浮点数指数位可能不一样，所以需要把两个指数位变成一样的。也就是**先对奇，再计算**。把两个指数统一成其中较大的-1，然后计算两者相加的有效位。**问题**：指数位较小的数，再进行有效位进行右移后，最右侧有效位被丢弃，得到对应指数位较小的数，在加法之前，就丢失精度。**两个相加位指数位差的越大，位移位数越大，可能丢失精度也就越大**，也就是大数吃小数的情况，这种场景在机器学习中，计算海量样本经常出现

**Kahan Summation**
在每次计算中，都用一次减法，把当前加法计算中损失的精度记录下来，然后在后面的循环中，把精度损失放在要加的小数上，再做一次运算。

## 指令的执行

**指令周期**

1. 取得指令，从PC寄存器找出对应指令地址，根据指令从内存把具体指令，加载到指令寄存器中，然后把PC寄存器自增。
2. 指令译码，解析指令寄存器指令为R,I,J种类指令，具体操作，具体操作寄存器，数据和内存地址
3. 执行，运行R,I,J指令，进行算术逻辑，数据传输或直接地址跳转

上述步骤，称为一个指令周期，不断循环。取指令从主内存读取机器码到控制器，控制器将指令解析成不同操作信号以及地址和操作数，执行指令ALU根据信号和数据进行实际运算。

- **机器周期/CPU周期**，从内存里面读取一条指令最短时间，称为CPU周期。

CPU周期通常由几个时钟周期累积起来。指令周期执行至少要两个CPU周期，取出指令至少要一个CPU周期，执行至少也要一个CPU周期。

**数据通路**

- 操作原件：组合逻辑元件，ALU。在特定输入下，根据组合电路逻辑，生成特定输出
- 存储原件：状态原件，计算过程用到的寄存器。

通过数据总线，把它们连接起来完成数据的存储，处理和传输，也就是**建立数据通路**。

所有CPU支持的指令，都会在控制器里，被解析成不同输出信号。运算器ALU和各种组合逻辑电路根据信号做不同计算。

## 时序逻辑电路
组合逻辑电路：根据给定输入，得到固定输出
时序逻辑电路：1.时序电路接通后可以不停开启和关闭开关；让PC寄存器自增读取成为可能。2.通过触发器，把计算结果存储在特定电路里面，3按照时间顺序让不同的事件发生。

**时钟信号硬件实现**
CPU主频是由晶体振荡器生成的电路信号，也就是时钟信号。该电路的实现通过电磁效应产生。

![](https://yatesblog.oss-cn-shenzhen.aliyuncs.com/img/computer-system-Perspective/41.png)

开关A和开关B互相通电（产生磁性），断电（失去磁性）产生0和1这样的信号，也就是时钟信号。相当于把电路输出信号作为输入信号，再回到当前电路，称之为**反馈电路**

**D触发器实现存储功能**
利用时钟信号和门电路组合，实现状态记忆功能。电路输出信号不单单取决于当前输入信号，还取决于输出信号之前的状态，也就是D触发器，实际在CPU内实现存储功能的寄存器的实现方式。

**程序计数器**
加法器两个输入，一个始终为1，另一个来自D触发器A。把加法器输出结果写到D触发器A里，那么D触发器的数据就会在固定时钟信号为1时更新一次。这样就可以有一个程序计数器了。

每次自增后，去对应D型触发器取值，也就是下一条需要运行指令地址（**同一个程序指令顺序存放内存里**）；为了保证在**一个时钟周期里，确保执行完一条最复杂的CPU指令**，于是有了**单指令周期处理器**

**译码器**
从输入多个位信号中，根据一定开关和电路组合，选择自己想要的信号。除了寻址，还需对应运行指令码，通过译码器，找出期望执行指令。

**一条指令执行流程**
![](https://yatesblog.oss-cn-shenzhen.aliyuncs.com/img/computer-system-Perspective/42.png)


1. 自动计数器会随着时钟主频不断地自增，作为PC寄存器。
2. 自动计数器的后面连上一个译码器。译码器同时连着大量的D触发器组成的内存。
3. 自动计数器会随着时钟主频不断自增，从译码器找到对应的计数器表示的内存地址，读取出CPU指令。
4. 读取出来的CPU指令通过CPU时钟的控制，写入到一个由D触发器组成的指令寄存器当中。
5. 在指令寄存器后面再跟一个译码器。这个译码器把我们拿到的指令解析成opcode和对应的操作数。
6. 拿到对应的opcode和操作数，输出线路连接ALU，进行各种算术和逻辑运算。计算结果再写回到D触发器组成的寄存器或者内存当中。

- if...else编程两条指令而不是一个复杂的电路变成一条指令。匹配当前“译码-执行-更新寄存器”的步骤
- 执行一条指令，可以不放在一个时钟周期内，可拆分到多个时钟周期

单指令周期处理器的缺点是无轮执行一条用不到ALU的无条件跳转指令，还是一条计算特别复杂的浮点数乘法运算，都要等满一个时钟周期。不然时钟频率太高会造成指令无法顺序执行，指令读取数据不准确。

## **指令流水线**
如果我们把一个指令分拆为“取指令-指令译码-执行指令”三个部分，如果执行指令又拆分为“ALU计算-内存访问-数据写回”，那么这就是一个5级流水线。CPU主频就可以提高了。**不需要确保最复杂那条指令在时钟周期里执行完成，而只要保障一个最复杂的流水线级操作，在一个时钟周期内完成就好了**。现代ARM或Intel的cpu，流水线级数都已达14级。流水线技术并不能缩短单条指令响应时间，但是可以增加运行多条指令时吞吐率。

### **流水线性能瓶颈**
每一级流水线对应输出，都要放到**流水线寄存器**，下一个时钟周期，交给下一个流水线级处理。如果不断加深流水线，操作占整个指令执行时间比例会不断增加。多出在overhead的开销。

不能简单通过CPU的主频，就衡量CPU乃至计算机整机性能，不同CPU实际体系架构和实现不一样

pentium 4成为Intel在技术架构层面失败，第一时晶体管增加造成的功耗，散热问题。第二是**冒险**问题，

### **结构冒险**
内存只有一个地址译码器作为地址输入，也就是说两个不同指令无法在一个时钟周期里，操作同一个数据，叫做**资源冲突**。为了解决这种问题，只有增加资源，存放指令的**程序内存**和存放数据的**数据内存**。也就是哈佛架构。而我们常说的冯.诺依曼架构没有这样做，是因为这样对内存空间失去了动态分配的灵活性。但是现代CPU利用高速缓存，分成了指令缓存和数据缓存。

### **数据冒险**
真正编程过程中，后一个指令**计算结果依赖前一个计算结果**，导致指令执行速度无法提升。**流水线越长，冒险问题更难解决**，如果只有3级流水，计算机可以使用**乱序执行技术**。
```java
    // 数据依赖  
      int a = 1;
      int b = 2;
      a = a + 2;
      b = a + 3;
    // 反依赖
      int a = 1;
      int b = 2;
      a = b + a;
      b = a + b;
  // 输出依赖
      int a = 1;
      a = 2;
```

那么怎么解决数据冒险问题呢？

**NOP操作**
在进行指令译码时，若发现后面指令会依赖前面指令数据，触发冒险，那我们会让整个流水线停顿一个或多个周期。时钟信号在0和1之间自动切换，没法停顿，所以在指令之间插入一个NOP操作

插入过多的NOP操作，意味着我们的CPU总是在空转，有没有少插入一些NOP操作呢

**操作数前推（盘路）**
如果我们两条有依赖的指令，前一条指令结果能够直接传输给第二条指令，作为输入，那就不用从寄存器，把数据再单独读出来一次，也不用插入两个NOP操作

**乱序执行**
很多时候，流水线不得不“阻塞”在特定的指令上
```java
a = b + c
d = a * e
x = y * z
```
计算x并不需要等待a和d计算完成
![](https://yatesblog.oss-cn-shenzhen.aliyuncs.com/img/computer-system-Perspective/43.png)

乱序执行，则是在指令执行的阶段通过一个类似线程池的保留站，让系统自己去动态调度先执行哪些指令。这个动态调度巧妙地解决了流水线阻塞的问题.指令执行的先后顺序，不再和它们在程序中的顺序有关。我们只要保证不破坏数据依赖就好了。CPU只要等到在指令结果的最终提交的阶段，再通过重排序的方式，确保指令“实际上”是顺序执行的
![](https://yatesblog.oss-cn-shenzhen.aliyuncs.com/img/computer-system-Perspective/44.png)


### **控制冒险**
如果遇到cmp，jmp和jle这种条件跳转指令时，后面的指令是否应该顺序加载。为了确保取到正确的指令，不得不进行等待延迟的情况，叫做**控制冒险**

那么怎么解决该问题呢

**缩短分支延迟**
条件跳转指令在执行时进行两种操作，1条件比较，根据指令opcode确认条件码寄存器 2实际跳转，把跳转地址信息写入PC寄存器。

和数据冒险的操作数前推方案类似。将条件判断，地址跳转，提前到**指令译码阶段**，不需要放到指令执行阶段，同时cpu设计对应旁路。

但是下一条指令还是要等待前一个指令执行结果，才能进行跳转

**静态分支预测**
CPU预测，条件跳转一定不发生，也就是50正确率，如果预测正确，节省需要停顿下来等待时间，如果预测失败，丢弃后面已经取出指令并执行部分。在流水线里，称为**Zap或Flush**。

**动态分支预测**

- 一级分支预测：使用1byte，记录当前分支比较情况，预测下一次分支比较情况。也叫1比特饱和计数
- 状态机：如果连续两次比较情况都是一样结果，那么下一次结果几率更大。需要2个比特来记录对应的状态。也叫2比特饱和计数

```java
public class BranchPrediction {
    public static void main(String args[]) {        
        long start = System.currentTimeMillis();
        for (int i = 0; i < 100; i++) {
            for (int j = 0; j <1000; j ++) {
                for (int k = 0; k < 10000; k++) {
                }
            }
        }
        long end = System.currentTimeMillis();
        System.out.println("Time spent is " + (end - start));
                
        start = System.currentTimeMillis();
        for (int i = 0; i < 10000; i++) {
            for (int j = 0; j <1000; j ++) {
                for (int k = 0; k < 100; k++) {
                }
            }
        }
        end = System.currentTimeMillis();
        System.out.println("Time spent is " + (end - start) + "ms");
    }
}
```

两端代码因为分支预测，两者运行时间相差几倍，

## **多发射与超标量**

在公式  **程序的CPU执行时间 = 指令数 × CPI × Clock Cycle Time** 中，CPI的倒数，IPC，一个时钟周期里面能够执行的指令数，代表了CPU的吞吐率，在流水线架构中，无论怎么优惠，一个时钟周期最多执行一条指令，也就是IPC只能到1

一直到80386，CPU都没有专门的计算浮点数的电路。在指令乱序执行阶段其实是由多个ALU并行执行的，而取指令和指令译码也可以通过**增加硬件**得到并行的效果，这样我们在一个周期内就可以做到IPC大于1了

![](https://yatesblog.oss-cn-shenzhen.aliyuncs.com/img/computer-system-Perspective/45.png)

上述被称为**多发射，超标量**

- 多发射：同一时间将多条指令发射到不同的译码器或者后续处理的流水线中
- 超标量：在超标量CPU里，有多条并行的流水线

**超长指令字（VLIW）技术**
在编译的过程中，就把指令先后的依赖关系进行调整，使得一次可以取一个指令包。（安腾服务器未得到市场认可）

## 超线程
intel在pentium4这个CPU上，由于流水线太深，起不到效果，更深的流水线意味着同时流水线内指令就多，相互依赖关系越多，为了解决冒险问题，NOP操作也就越多。

同一时间点，一个物理CPU核心只会运行一个线程指令，无法做到真正指令并行运行。**超线程CPU（SMT）**，把一个物理层面CPU核心，伪装成两个逻辑层面CPU核心。物理层面会**增加很多电路**（双份PC寄存器，指令寄存器，条件码寄存器），而其他ALU，指令译码等组件并不会提供双份。**超线程的目的，是在一个线程A的指令，在流水线停顿的时候，让另外一个线程去执行指令。**而另一个线程对于线程A没有指令关联和依赖。

多线程技术在特定应用场景下效果比较好。一般是各个线程**等待**时间比较长的应用场景下。比如：数据库请求。

## SIMD 单指令多数据流

**SISD**：通过循环一步一步计算的算法我们称为，**单指令单数据**。

**MIMD**：如果手头是一个多核CPU，同时处理多个指令的方式我们称为，**多指令多数据**。

SIMD再获取数据和执行指令时都做到了并行，以一个循环读取**整数自增**的数组为例，每一项都是Integer，就是4
Bytes内存空间，Intel引入SSE指令集时，添加了8个16byte的寄存器，一次性加载4个整数。到了指令执行阶段，4个整数也是各自进行操作。SIMD在这个大量数据并行，向量运算/矩阵运算能大大提高计算效率。

## 异常
异常是一个硬件和软件组合到一起的处理过程。异常前半生，发生和捕捉，是在硬件层完成的；后半生，异常的处理是由软件完成的。

每一种可能发生的异常，都有对应异常代码。异常发生时，cpu检测到特殊信号，拿到对应异常代码。**I/O发出的信号异常代码，由操作系统分配，软件设定的。像加法溢出这样的异常代码，由CPU预先分配好的，由硬件分配的。这是一个软件和硬件共同组合来处理异常的过程**

异常代码处理：CPU拿到异常码后，保存当前程序执行现场到程序栈，cpu通过保留在内存里的异常表，找到异常处理程序，触发异常处理流程。

![](https://yatesblog.oss-cn-shenzhen.aliyuncs.com/img/computer-system-Perspective/46.png)

**异常分类**

- 中断：程序执行到一半，被打断。信号来自CPU外部I/O设备。
- 陷阱：主动触发，类似程序中打断点。用户态切换时，应用程序通过系统调用读取文件，创建进程，通过触发陷阱进行的。
- 故障：程序执行过程中，不在开发计划内发生异常，比如计算发生溢出。异常处理完成后，仍然回来处理当前指令，而不是下一条指令
- 中止：CPU遇到故障，恢复不过来时，程序就中止了。

中断信号来自系统外部，我们称为**异步类型异常**，而其他三种程序执行过程中发生，称为**同步类型异常**。中断，陷阱，故障处理流程都是“**保存现场，异常代码查询，异常处理程序调用**”。而中止时，发现没有异常处理程序可以处理该异常，程序不得不**退出当前程序执行**。

**保存现场**
保存现场操作跟函数调用相似，需将当前执行指令压栈，指令控制权切换到另一个“函数”里面。

- 异常发生在程序正常执行预期之外，除了指令压栈，还需把CPU当前运行程序所需用到所有寄存器，都放到栈里面（条件码寄存器内容）。
- 陷阱之类异常，涉及用户态和内核态切换，压栈数据是到内核栈，而不是程序栈
- 故障之类异常，异常处理完后，栈里返回出来，继续执行故障发生当前指令。

异常处理流程，不像是顺序执行指令间函数调用关系，更像是两个不同独立进程之间在CPU层面切换，我们称为**上下文切换**

## 指令集

- 复杂指令集：CPU的指令集里机器码是可变长度的，简称CISC。
- 精简指令集：CPU的指令集里机器码是固定长度的，简称RISC

![](https://yatesblog.oss-cn-shenzhen.aliyuncs.com/img/computer-system-Perspective/47.png)

硬件方面，想要**支持更多复杂指令**，CPU里面**电路就更复杂**，设计就更困难。**散热，功耗**也会是挑战。软件层面，支持更多复杂指令，**编译器优化**就更困难。RISC架构，精简到20%简单指令，复杂指令通过简单指令组合实现，软件实现硬件的功能。CPU整个硬件设计会更简单，硬件层面提升性能更容易了。腾出空间可用于放**通用寄存器**（由于执行指令数比CISC多，反复从内存读取指令或数据到寄存器，会有访问内存开销）。

无论什么指令集架构，都有一个最重要的考量，就是h指令集向前兼容性。intel在pentium pro开始使用**微指令架构**。

微指令架构里，编译器编译出来机器码和汇编代码并没有发生什么变化。指令译码阶段，译码器把机器码翻译成多条固定长度的RISC风格的微指令，微指令被放到缓冲区内，在分发到超标量，且乱序执行流水线架构。指令译码器此时相当于一个适配器，适配了CISC和RISC之间指令差异。为了避免指令译码的开销浪费，加了一层L0cache，保存指令译码器把CISC指令翻译成RISC的微指令结果。优化了性能，还减少了功耗。

虽然微指令存在提高了性能，但由于Intel本身在CPU层面做的大量优化，比如乱序执行，分支预测，X86的CPU始终功耗还是远远超过RISC架构的ARM。开源的RISC-V是CPU的未来。

## GPU

**图形渲染流程**

- 顶点处理：构成多边形建模的每一个多边形都有多个顶点。顶点在三维空间，确定视角时，我们需要把顶点从三维空间转化到屏幕二维空间内。**每一个顶点位置转化，互相之间没有依赖，可以并行独立计算**第三维变成了屏幕的深度
- 图元处理：把处理后顶点连起来，变成多边形。针对多边形，把不在或一部分不在屏幕里面内容去掉，这个过程叫剔除和裁剪
- 栅格化：把多边形转换为屏幕一个个像素点。也就是把每个**图元并行独立地栅格化**
- 片段处理：计算每一个像素的颜色，透明度，给像素上色。**每个片段并行，独立进行**
- 像素操作：把不同多边形像素点混合到一起，前后多边形颜色的混合/覆盖。

上面流程称为**图形流水线**

早期顶点处理是在CPU进行的，后面移到GPU里面。

**可编程管线**
起初，整个图形渲染过程是在硬件里固定管线来完成的。只有通过改配置来实现不同图形渲染效果。引入可编程管线概念，使GPU也能有一定可编程能力，在**整个渲染管线一些特别步骤，能够定义处理数据的算法或操作**。早期用语言顶点处理和片段处理，两个不同的shader，为了优化电路的运行，统一为**统一着色器架构**。


![](https://yatesblog.oss-cn-shenzhen.aliyuncs.com/img/computer-system-Perspective/48.png)

shader变成统一模块，才有了CPU拿来做各种计算的用法，**GPGPU**，才有了机器学习的火热


GPU作为只进行流失处理过程，只保留了**取指令，指令译码，ALU以及执行计算所需寄存器和缓存**，实现了**计算**这个核心功能。对于图形的处理流程，对每个点计算都是处理的，天然的支持并行加速。借鉴CPU的SIMD，创造出SIMT，把多条数据，交给不同线程去处理。各个线程执行指令流程都一样，但是指令可能不同，所走分支不一样，两个线程走不通分支。

![](https://yatesblog.oss-cn-shenzhen.aliyuncs.com/img/computer-system-Perspective/49.png)

CPU的指令也会遇到流水线停顿问题，和超线程一样，调度其他计算任务给当前ALU，针对任务，提供更多的执行上下文。

**FPGA**
CPU里面有上亿晶体管，设计，验证一个芯片技术方案，不可能一个个的造出验证。于是有了FPGA，**线程可编程门阵列**

**ASIC**
专用集成电路，为专门用途场景设计的芯片。

FPGA没有硬件成本。ASIC电路设计，仿真，验证，流片，最终印刷为电路板，变成芯片，研发到上市，最低花费也要几万美元。但是单个ASIC生产成本比FPGA低，能耗也比GPGA低。只有大量ASIC芯片生产，才能降低总体拥有成本。

## TPU
深度学习的**推断**工作比训练要简单，对灵活性要求更低；它保障响应时间的指标，功耗上更少。

![](https://yatesblog.oss-cn-shenzhen.aliyuncs.com/img/computer-system-Perspective/50.png)

一个深度学习推断过程，由很多层计算组成。每一层计算过程就是先进行矩阵乘法，再累加，调用激活函数，最后归一化和池化

## 虚拟机
指在现有硬件的操作系统上，能够模拟一个计算机系统的技术。称为模拟器，而这个模拟器实现最简单的方法就是兼容这个计算机系统指令集。
模拟器的优点是，模拟的系统可以跨硬件。但是做不到100%模拟。其次这种解释执行方式，性能太差（不是直接把指令交给CPU执行，而是经过各种解释和翻译工作）。

现在我们租用的阿里云，腾讯云的服务器资源都是虚拟机。通过咋宿主机操作系统运行一个虚拟机监控器，然后再在虚拟机监控器上运行客户机操作系统，可分为Type-1和Type-2类型：

- type1:实际指令不需要通过宿主机操作系统，而是直接通过虚拟机监视器访问硬件
- type2:所有指令需经历客户机操作系统，虚拟机监视器，宿主机操作系统，性能比Type1慢。但是兼容性要好一些。

多个type-1型虚拟机技术存在一个物理机会造成操作系统浪费，轻量级Docker技术通过隔离资源来做到操作系统级的虚拟机技术。